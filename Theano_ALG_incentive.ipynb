{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, aelosscoef = 0.1):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, 4, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encode, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "        self.actionlayer = lasagne.layers.InputLayer(shape=(None, self.n_actions), input_var = self.actions_onehot)\n",
    "        self.prins = lasagne.layers.ConcatLayer([self.encode, self.actionlayer], axis = 1)\n",
    "        self.pr1 = lasagne.layers.DenseLayer(self.prins, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.prencode = lasagne.layers.DenseLayer(self.pr1, 4)\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc, self.prenc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode, self.prencode])\n",
    "        self.targetenc = lasagne.layers.get_output(self.encode, inputs = self.newstates)\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "        \n",
    "        self.pr_loss_batch = T.mean(T.sqr(self.targetenc - self.prenc), axis = 1)\n",
    "        self.pr_loss = T.mean(self.pr_loss_batch)\n",
    "        pr_params = [self.pr1.W, self.pr1.b, self.prencode.W, self.prencode.b]\n",
    "        self.pr_grads = T.grad(self.pr_loss, pr_params)\n",
    "        self.pr_scaled_grads = lasagne.updates.total_norm_constraint(self.pr_grads, grad_clipping)\n",
    "        self.pr_updates = lasagne.updates.adam(self.pr_scaled_grads, pr_params, learning_rate=lr)\n",
    "        \n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_predfn = theano.function([self.input_var, self.actions, self.newstates], self.pr_loss_batch, updates = self.pr_updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "        self.maxprerror = None\n",
    "        \n",
    "        self.env = env\n",
    "        self.mainQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "        self.targetQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                        if self.maxprerror == None:\n",
    "                            self.maxprerror = np.max(errors_vector)\n",
    "                        else:\n",
    "                            self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                        rewards_vector = rewards_vector + errors_vector * 50.0 / (self.maxprerror * self.total_steps)\n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                    \n",
    "                    if self.maxprerror == None:\n",
    "                        self.maxprerror = np.max(errors_vector)\n",
    "                    else:\n",
    "                        self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-01 05:04:22,857] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cpenv = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 1,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cpenv, egreedyagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 36841\n",
      "Episode rewards, last 10: [500.0, 186.0, 84.0, 31.0, 500.0, 500.0, 500.0, 500.0, 500.0, 109.0]\n",
      "Mean over last 500 episodes: 83.682\n",
      "Episode lengths, last 10: [499, 185, 83, 30, 499, 499, 499, 499, 499, 108]\n",
      "AEerror, mean over last 10: 0.173185683523\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 133380\n",
      "Episode rewards, last 10: [30.0, 28.0, 154.0, 143.0, 72.0, 117.0, 35.0, 108.0, 15.0, 60.0]\n",
      "Mean over last 500 episodes: 193.078\n",
      "Episode lengths, last 10: [29, 27, 153, 142, 71, 116, 34, 107, 14, 59]\n",
      "AEerror, mean over last 10: 0.514058912617\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 237148\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 16.0, 11.0, 119.0, 58.0, 16.0]\n",
      "Mean over last 500 episodes: 207.536\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 15, 10, 118, 57, 15]\n",
      "AEerror, mean over last 10: 0.267304757364\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 346507\n",
      "Episode rewards, last 10: [12.0, 13.0, 83.0, 86.0, 78.0, 18.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 218.718\n",
      "Episode lengths, last 10: [11, 12, 82, 85, 77, 17, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.411065790981\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 481345\n",
      "Episode rewards, last 10: [387.0, 231.0, 297.0, 246.0, 165.0, 163.0, 182.0, 12.0, 189.0, 149.0]\n",
      "Mean over last 500 episodes: 269.676\n",
      "Episode lengths, last 10: [386, 230, 296, 245, 164, 162, 181, 11, 188, 148]\n",
      "AEerror, mean over last 10: 0.51741769813\n",
      "=======================================================================\n",
      "========= Episode 2999 ================================================\n",
      "Total steps: 610487\n",
      "Episode rewards, last 10: [96.0, 13.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 89.0, 24.0]\n",
      "Mean over last 500 episodes: 258.284\n",
      "Episode lengths, last 10: [95, 12, 499, 499, 499, 499, 499, 499, 88, 23]\n",
      "AEerror, mean over last 10: 0.49461852759\n",
      "=======================================================================\n",
      "========= Episode 3499 ================================================\n",
      "Total steps: 735471\n",
      "Episode rewards, last 10: [418.0, 203.0, 383.0, 299.0, 322.0, 367.0, 320.0, 500.0, 401.0, 98.0]\n",
      "Mean over last 500 episodes: 249.968\n",
      "Episode lengths, last 10: [417, 202, 382, 298, 321, 366, 319, 499, 400, 97]\n",
      "AEerror, mean over last 10: 0.649611250278\n",
      "=======================================================================\n",
      "========= Episode 3999 ================================================\n",
      "Total steps: 843407\n",
      "Episode rewards, last 10: [14.0, 32.0, 13.0, 44.0, 35.0, 185.0, 290.0, 285.0, 257.0, 426.0]\n",
      "Mean over last 500 episodes: 215.872\n",
      "Episode lengths, last 10: [13, 31, 12, 43, 34, 184, 289, 284, 256, 425]\n",
      "AEerror, mean over last 10: 0.427841495549\n",
      "=======================================================================\n",
      "========= Episode 4499 ================================================\n",
      "Total steps: 954112\n",
      "Episode rewards, last 10: [69.0, 66.0, 67.0, 60.0, 63.0, 61.0, 57.0, 62.0, 53.0, 39.0]\n",
      "Mean over last 500 episodes: 221.41\n",
      "Episode lengths, last 10: [68, 65, 66, 59, 62, 60, 56, 61, 52, 38]\n",
      "AEerror, mean over last 10: 0.562918323631\n",
      "=======================================================================\n",
      "========= Episode 4999 ================================================\n",
      "Total steps: 1033947\n",
      "Episode rewards, last 10: [10.0, 13.0, 10.0, 11.0, 16.0, 10.0, 240.0, 10.0, 78.0, 13.0]\n",
      "Mean over last 500 episodes: 159.67\n",
      "Episode lengths, last 10: [9, 12, 9, 10, 15, 9, 239, 9, 77, 12]\n",
      "AEerror, mean over last 10: 0.709061943733\n",
      "=======================================================================\n",
      "========= Episode 5499 ================================================\n",
      "Total steps: 1120759\n",
      "Episode rewards, last 10: [69.0, 12.0, 59.0, 53.0, 54.0, 13.0, 46.0, 18.0, 12.0, 19.0]\n",
      "Mean over last 500 episodes: 173.624\n",
      "Episode lengths, last 10: [68, 11, 58, 52, 53, 12, 45, 17, 11, 18]\n",
      "AEerror, mean over last 10: 0.336292546814\n",
      "=======================================================================\n",
      "========= Episode 5999 ================================================\n",
      "Total steps: 1262941\n",
      "Episode rewards, last 10: [500.0, 500.0, 425.0, 26.0, 28.0, 500.0, 500.0, 500.0, 13.0, 33.0]\n",
      "Mean over last 500 episodes: 284.364\n",
      "Episode lengths, last 10: [499, 499, 424, 25, 27, 499, 499, 499, 12, 32]\n",
      "AEerror, mean over last 10: 0.495379591169\n",
      "=======================================================================\n",
      "========= Episode 6499 ================================================\n",
      "Total steps: 1381913\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 88.0, 27.0, 28.0, 28.0, 16.0, 500.0]\n",
      "Mean over last 500 episodes: 237.944\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 87, 26, 27, 27, 15, 499]\n",
      "AEerror, mean over last 10: 0.333073590039\n",
      "=======================================================================\n",
      "========= Episode 6999 ================================================\n",
      "Total steps: 1487749\n",
      "Episode rewards, last 10: [140.0, 147.0, 179.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 211.672\n",
      "Episode lengths, last 10: [139, 146, 178, 499, 499, 499, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.855600885407\n",
      "=======================================================================\n",
      "========= Episode 7499 ================================================\n",
      "Total steps: 1613581\n",
      "Episode rewards, last 10: [49.0, 52.0, 30.0, 49.0, 54.0, 207.0, 225.0, 148.0, 145.0, 36.0]\n",
      "Mean over last 500 episodes: 251.664\n",
      "Episode lengths, last 10: [48, 51, 29, 48, 53, 206, 224, 147, 144, 35]\n",
      "AEerror, mean over last 10: 0.103385759705\n",
      "=======================================================================\n",
      "========= Episode 7999 ================================================\n",
      "Total steps: 1751308\n",
      "Episode rewards, last 10: [500.0, 500.0, 12.0, 19.0, 500.0, 500.0, 500.0, 500.0, 500.0, 447.0]\n",
      "Mean over last 500 episodes: 275.454\n",
      "Episode lengths, last 10: [499, 499, 11, 18, 499, 499, 499, 499, 499, 446]\n",
      "AEerror, mean over last 10: 0.238642235066\n",
      "=======================================================================\n",
      "========= Episode 8499 ================================================\n",
      "Total steps: 1781557\n",
      "Episode rewards, last 10: [11.0, 9.0, 10.0, 9.0, 10.0, 10.0, 9.0, 10.0, 13.0, 13.0]\n",
      "Mean over last 500 episodes: 60.498\n",
      "Episode lengths, last 10: [10, 8, 9, 8, 9, 9, 8, 9, 12, 12]\n",
      "AEerror, mean over last 10: 0.861260824682\n",
      "=======================================================================\n",
      "========= Episode 8999 ================================================\n",
      "Total steps: 1786624\n",
      "Episode rewards, last 10: [12.0, 11.0, 8.0, 10.0, 8.0, 10.0, 9.0, 9.0, 9.0, 12.0]\n",
      "Mean over last 500 episodes: 10.134\n",
      "Episode lengths, last 10: [11, 10, 7, 9, 7, 9, 8, 8, 8, 11]\n",
      "AEerror, mean over last 10: 0.687818378166\n",
      "=======================================================================\n",
      "========= Episode 9499 ================================================\n",
      "Total steps: 1840129\n",
      "Episode rewards, last 10: [159.0, 124.0, 130.0, 140.0, 137.0, 21.0, 143.0, 128.0, 128.0, 119.0]\n",
      "Mean over last 500 episodes: 107.01\n",
      "Episode lengths, last 10: [158, 123, 129, 139, 136, 20, 142, 127, 127, 118]\n",
      "AEerror, mean over last 10: 0.741981740435\n",
      "=======================================================================\n",
      "========= Episode 9999 ================================================\n",
      "Total steps: 1951918\n",
      "Episode rewards, last 10: [83.0, 75.0, 10.0, 9.0, 63.0, 147.0, 171.0, 108.0, 99.0, 163.0]\n",
      "Mean over last 500 episodes: 223.578\n",
      "Episode lengths, last 10: [82, 74, 9, 8, 62, 146, 170, 107, 98, 162]\n",
      "AEerror, mean over last 10: 0.147432300972\n",
      "=======================================================================\n",
      "========= Episode 10499 ================================================\n",
      "Total steps: 2091212\n",
      "Episode rewards, last 10: [77.0, 62.0, 93.0, 74.0, 64.0, 56.0, 77.0, 14.0, 73.0, 173.0]\n",
      "Mean over last 500 episodes: 278.588\n",
      "Episode lengths, last 10: [76, 61, 92, 73, 63, 55, 76, 13, 72, 172]\n",
      "AEerror, mean over last 10: 0.170122850819\n",
      "========================================================================\n",
      "========= Episode 10999 ================================================\n",
      "Total steps: 2202560\n",
      "Episode rewards, last 10: [33.0, 32.0, 29.0, 30.0, 32.0, 32.0, 30.0, 31.0, 26.0, 32.0]\n",
      "Mean over last 500 episodes: 222.696\n",
      "Episode lengths, last 10: [32, 31, 28, 29, 31, 31, 29, 30, 25, 31]\n",
      "AEerror, mean over last 10: 0.599035625537\n",
      "========================================================================\n",
      "========= Episode 11499 ================================================\n",
      "Total steps: 2229602\n",
      "Episode rewards, last 10: [10.0, 9.0, 10.0, 10.0, 9.0, 10.0, 12.0, 10.0, 10.0, 8.0]\n",
      "Mean over last 500 episodes: 54.084\n",
      "Episode lengths, last 10: [9, 8, 9, 9, 8, 9, 11, 9, 9, 7]\n",
      "AEerror, mean over last 10: 0.725277356239\n",
      "========================================================================\n",
      "========= Episode 11999 ================================================\n",
      "Total steps: 2234506\n",
      "Episode rewards, last 10: [9.0, 8.0, 9.0, 9.0, 8.0, 12.0, 8.0, 9.0, 12.0, 11.0]\n",
      "Mean over last 500 episodes: 9.808\n",
      "Episode lengths, last 10: [8, 7, 8, 8, 7, 11, 7, 8, 11, 10]\n",
      "AEerror, mean over last 10: 0.801322679011\n",
      "========================================================================\n",
      "========= Episode 12499 ================================================\n",
      "Total steps: 2239432\n",
      "Episode rewards, last 10: [8.0, 10.0, 10.0, 10.0, 12.0, 9.0, 10.0, 9.0, 8.0, 9.0]\n",
      "Mean over last 500 episodes: 9.852\n",
      "Episode lengths, last 10: [7, 9, 9, 9, 11, 8, 9, 8, 7, 8]\n",
      "AEerror, mean over last 10: 0.74352085468\n",
      "========================================================================\n",
      "========= Episode 12999 ================================================\n",
      "Total steps: 2244376\n",
      "Episode rewards, last 10: [12.0, 8.0, 11.0, 10.0, 9.0, 9.0, 10.0, 11.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.888\n",
      "Episode lengths, last 10: [11, 7, 10, 9, 8, 8, 9, 10, 9, 9]\n",
      "AEerror, mean over last 10: 0.660808231808\n",
      "========================================================================\n",
      "========= Episode 13499 ================================================\n",
      "Total steps: 2249348\n",
      "Episode rewards, last 10: [9.0, 11.0, 9.0, 9.0, 8.0, 10.0, 10.0, 9.0, 9.0, 9.0]\n",
      "Mean over last 500 episodes: 9.944\n",
      "Episode lengths, last 10: [8, 10, 8, 8, 7, 9, 9, 8, 8, 8]\n",
      "AEerror, mean over last 10: 0.805068694783\n",
      "========================================================================\n",
      "========= Episode 13999 ================================================\n",
      "Total steps: 2254277\n",
      "Episode rewards, last 10: [9.0, 13.0, 9.0, 10.0, 10.0, 9.0, 10.0, 8.0, 11.0, 12.0]\n",
      "Mean over last 500 episodes: 9.858\n",
      "Episode lengths, last 10: [8, 12, 8, 9, 9, 8, 9, 7, 10, 11]\n",
      "AEerror, mean over last 10: 0.769395843082\n",
      "========================================================================\n",
      "========= Episode 14499 ================================================\n",
      "Total steps: 2259215\n",
      "Episode rewards, last 10: [9.0, 10.0, 12.0, 11.0, 10.0, 9.0, 13.0, 10.0, 10.0, 13.0]\n",
      "Mean over last 500 episodes: 9.876\n",
      "Episode lengths, last 10: [8, 9, 11, 10, 9, 8, 12, 9, 9, 12]\n",
      "AEerror, mean over last 10: 0.773988980629\n",
      "========================================================================\n",
      "========= Episode 14999 ================================================\n",
      "Total steps: 2264199\n",
      "Episode rewards, last 10: [8.0, 9.0, 9.0, 10.0, 9.0, 11.0, 12.0, 8.0, 11.0, 12.0]\n",
      "Mean over last 500 episodes: 9.968\n",
      "Episode lengths, last 10: [7, 8, 8, 9, 8, 10, 11, 7, 10, 11]\n",
      "AEerror, mean over last 10: 0.666966624038\n",
      "========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddql.train(num_episodes = 15000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxWFDzzAMY3HY0DMMw1ic/w8brNk9eAXscgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11935b290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "График наград у стратегии исследования с поощрением исследования. Так как эта стратегия влияет на работу агента модифицируя награды у наборов (s_t, a_t, r_t, s_t+1), то с этим алгоритмом применимы предыдущие стратегии исследования(е-жадная и больцман). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "70f8c23ca0cc4afa9cd243db5c41ae84": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
