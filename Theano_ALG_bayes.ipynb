{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, h = 50, alpha = 25):\n",
    "        self.input_var = T.tensor4('input')\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape)\n",
    "        self.build_AVQ(grad_clipping, lr, h, alpha)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 50, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr, h, alpha):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.l_varq = lasagne.layers.DenseLayer(self.outlayer, self.n_actions, nonlinearity=lasagne.nonlinearities.softplus)\n",
    "        \n",
    "        self.advantage, self.value, self.varq = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_varq])\n",
    "\n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Q = self.advantage + self.value - self.average_advantage\n",
    "        self.Q = self.advantage\n",
    "        self.predict = T.argmax(self.Q, axis = 1)\n",
    "        \n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, self.n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.Q0 = T.sum(self.Q * self.actions_onehot, axis = 1)\n",
    "        self.varQ0 = T.sum(self.varq * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q1 = self.Q0 + (self.targetQ - self.Q0) / (h + 1)\n",
    "        self.varQ1 = (h * (alpha - 1)) / ((h + 1) * (alpha - 0.5)) * \\\n",
    "                     (self.varQ0 + T.sqr(self.targetQ - self.Q0) / (2 * (h+1) * (alpha - 1)))\n",
    "        \n",
    "        self.td_error = T.mean(T.sqr(self.Q1 - self.Q0))\n",
    "        self.var_error = T.mean(T.sqr(self.varQ1 - self.varQ0))\n",
    "        self.loss = self.td_error + self.var_error\n",
    "        \n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Q)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.loss, self.Q1, self.varQ1], updates = self.updates)\n",
    "        self.var_fn = theano.function([self.input_var], self.varq)\n",
    "    \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params([self.l_advantage, self.l_varq], trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_varq], trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_varq], values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bayes_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, var_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "        self.var_fn = var_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            scores = self.var_fn(np.expand_dims(state, axis = 0))\n",
    "            a = np.argmax(scores)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.h = lparams[\"lambda\"]\n",
    "        self.alpha = lparams[\"alpha\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "        \n",
    "        self.env = env\n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, \n",
    "                      self.grad_clip, self.lr, self.h, self.alpha]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.jList = []\n",
    "        self.rList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            return egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            return boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        elif agent == \"bayes\":\n",
    "            return bayes_agent(self.env.n_actions, self.mainQN.actionpred_fn, self.mainQN.var_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "\n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "            \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "                \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                self.experience_storage.add(np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5]))\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)   \n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        loss, Qval, Qvar = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                         \n",
    "                else:\n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            self.jList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.jList[-10:])]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.jList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-02 04:54:17,991] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cpenv = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 1,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"lambda\": 4,\n",
    "           \"alpha\": 2,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayesagentinfo = {\"agent\" : \"bayes\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cpenv, bayesagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 0\n",
      "Episode rewards, last 10: [9.0, 9.0, 10.0, 10.0, 9.0, 9.0, 9.0, 10.0, 10.0, 8.0]\n",
      "Mean over last 500 episodes: 9.458\n",
      "Episode lengths, last 10: [8, 8, 9, 9, 8, 8, 8, 9, 9, 7]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 94836\n",
      "Episode rewards, last 10: [156.0, 259.0, 176.0, 126.0, 154.0, 176.0, 194.0, 245.0, 180.0, 186.0]\n",
      "Mean over last 500 episodes: 190.214\n",
      "Episode lengths, last 10: [155, 258, 175, 125, 153, 175, 193, 244, 179, 185]\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 203528\n",
      "Episode rewards, last 10: [56.0, 42.0, 21.0, 14.0, 17.0, 61.0, 104.0, 106.0, 37.0, 99.0]\n",
      "Mean over last 500 episodes: 217.384\n",
      "Episode lengths, last 10: [55, 41, 20, 13, 16, 60, 103, 105, 36, 98]\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 322964\n",
      "Episode rewards, last 10: [488.0, 351.0, 143.0, 500.0, 412.0, 274.0, 16.0, 37.0, 65.0, 281.0]\n",
      "Mean over last 500 episodes: 238.872\n",
      "Episode lengths, last 10: [487, 350, 142, 499, 411, 273, 15, 36, 64, 280]\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 415595\n",
      "Episode rewards, last 10: [159.0, 198.0, 144.0, 135.0, 152.0, 142.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 185.262\n",
      "Episode lengths, last 10: [158, 197, 143, 134, 151, 141, 499, 499, 499, 499]\n",
      "=======================================================================\n",
      "========= Episode 2999 ================================================\n",
      "Total steps: 534808\n",
      "Episode rewards, last 10: [41.0, 37.0, 56.0, 34.0, 30.0, 30.0, 30.0, 19.0, 22.0, 28.0]\n",
      "Mean over last 500 episodes: 238.426\n",
      "Episode lengths, last 10: [40, 36, 55, 33, 29, 29, 29, 18, 21, 27]\n",
      "=======================================================================\n",
      "========= Episode 3499 ================================================\n",
      "Total steps: 644229\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 321.0, 90.0, 363.0, 16.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 218.842\n",
      "Episode lengths, last 10: [499, 499, 499, 320, 89, 362, 15, 499, 499, 499]\n",
      "=======================================================================\n",
      "========= Episode 3999 ================================================\n",
      "Total steps: 725122\n",
      "Episode rewards, last 10: [15.0, 95.0, 500.0, 117.0, 500.0, 500.0, 500.0, 500.0, 500.0, 426.0]\n",
      "Mean over last 500 episodes: 161.786\n",
      "Episode lengths, last 10: [14, 94, 499, 116, 499, 499, 499, 499, 499, 425]\n",
      "=======================================================================\n",
      "========= Episode 4499 ================================================\n",
      "Total steps: 840310\n",
      "Episode rewards, last 10: [291.0, 372.0, 500.0, 500.0, 9.0, 42.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 230.376\n",
      "Episode lengths, last 10: [290, 371, 499, 499, 8, 41, 499, 499, 499, 499]\n",
      "=======================================================================\n",
      "========= Episode 4999 ================================================\n",
      "Total steps: 969022\n",
      "Episode rewards, last 10: [484.0, 500.0, 92.0, 186.0, 18.0, 15.0, 20.0, 11.0, 13.0, 16.0]\n",
      "Mean over last 500 episodes: 257.424\n",
      "Episode lengths, last 10: [483, 499, 91, 185, 17, 14, 19, 10, 12, 15]\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddql.train(num_episodes = 5000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEII4ZEECCGEEB5JgBBCCOHR/wMMrPTtN34ZSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125306990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Исследование окружение с использованием значения оценки выборочной дисперсии. Байесовский агент реализован как е-жадный агент, который с вероятностью 1-е выбирает оптимальное действие, а с вероятностью е выбирает то действие, Qvalue которого имеет максимальную дисперсию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "432fa012ac544616a7461cbbf7c565c0": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
