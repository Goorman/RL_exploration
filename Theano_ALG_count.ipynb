{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, \n",
    "                 encodeunits = 10, encodenoise = 0.1, aelosscoef = 0.1, regaelosscoef = 2.0):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.regaelosscoef = regaelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape, encodeunits, encodenoise)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape, encodeunits, encodenoise):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, encodeunits, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        self.encodenoise = lasagne.layers.GaussianNoiseLayer(self.encode, sigma = encodenoise)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encodenoise, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode])\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.regae_error = 0.25 - T.mean(T.sqr(self.enc - 0.5))\n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.regaelosscoef * self.regae_error + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "\n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        self.get_encode = theano.function([self.input_var], [self.enc, self.regae_error])\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.encode_unitnum = lparams[\"encode_unitnum\"]\n",
    "        self.encode_noise = lparams[\"encode_noise\"]\n",
    "        self.encode_reward_multiplier = lparams[\"encode_reward_multiplier\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "\n",
    "        \n",
    "        self.env = env\n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, \n",
    "                      self.lr, self.encode_unitnum, self.encode_noise]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.encode_counts = defaultdict(int)\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        encodes, reg_errors = self.mainQN.get_encode(old_state_batch)\n",
    "                        encodes = [\"\".join([str(k) for k in encode]) for encode in (encodes > 0.5).astype(int)]\n",
    "                        for encode in encodes:\n",
    "                            self.encode_counts[encode] += 1\n",
    "                        encode_rewards = self.encode_reward_multiplier / np.sqrt(np.array([self.encode_counts[encode] for encode in encodes]))\n",
    "                        rewards_vector = rewards_vector + encode_rewards \n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-02 04:59:26,511] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cpenv = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 3,\n",
    "           \"encode_unitnum\": 10,\n",
    "           \"encode_noise\": 0.05,\n",
    "           \"encode_reward_multiplier\": 10.0,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.06,\n",
    "                                \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cpenv, egreedyagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 21084\n",
      "Episode rewards, last 10: [65.0, 131.0, 117.0, 76.0, 119.0, 57.0, 180.0, 142.0, 156.0, 159.0]\n",
      "Mean over last 500 episodes: 52.168\n",
      "Episode lengths, last 10: [64, 130, 116, 75, 118, 56, 179, 141, 155, 158]\n",
      "AEerror, mean over last 10: 0.536348395423\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 117457\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 17.0, 48.0, 10.0, 30.0, 21.0, 83.0]\n",
      "Mean over last 500 episodes: 192.746\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 16, 47, 9, 29, 20, 82]\n",
      "AEerror, mean over last 10: 0.251910423848\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 234327\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 10.0, 12.0, 16.0, 16.0, 10.0, 14.0]\n",
      "Mean over last 500 episodes: 233.74\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 9, 11, 15, 15, 9, 13]\n",
      "AEerror, mean over last 10: 0.487676487977\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 348318\n",
      "Episode rewards, last 10: [172.0, 214.0, 500.0, 500.0, 411.0, 280.0, 272.0, 274.0, 13.0, 52.0]\n",
      "Mean over last 500 episodes: 227.982\n",
      "Episode lengths, last 10: [171, 213, 499, 499, 410, 279, 271, 273, 12, 51]\n",
      "AEerror, mean over last 10: 0.752276227837\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 457323\n",
      "Episode rewards, last 10: [101.0, 105.0, 101.0, 93.0, 13.0, 12.0, 13.0, 13.0, 28.0, 11.0]\n",
      "Mean over last 500 episodes: 218.01\n",
      "Episode lengths, last 10: [100, 104, 100, 92, 12, 11, 12, 12, 27, 10]\n",
      "AEerror, mean over last 10: 0.836893493262\n",
      "=======================================================================\n",
      "========= Episode 2999 ================================================\n",
      "Total steps: 551546\n",
      "Episode rewards, last 10: [500.0, 500.0, 217.0, 70.0, 65.0, 17.0, 66.0, 77.0, 70.0, 78.0]\n",
      "Mean over last 500 episodes: 188.446\n",
      "Episode lengths, last 10: [499, 499, 216, 69, 64, 16, 65, 76, 69, 77]\n",
      "AEerror, mean over last 10: 0.429267570711\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddql.train(num_episodes = 3000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nThK9EEL4OEn0Qgjh4yTRCyGEj/v/ZZqmMOGiQbYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126526350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм с учитыванием частоты посещения состояний. Подсчет посещения состояний определяется с использованием автоэнкодера, в котором все нейроны центрального слоя (с сигмоидной функцией активации) регуляризуются для помещения значений либо у нуля, либо у единицы. Эта регуляризация осуществляется с помощью изменения оптимизируемой функции потерь и с помощью добавления слоя гауссового шума после центрального слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стабильность подобной кодировки можно ожидать из-за того, что сигмоидные нейроны быстро насыщаются, предотвращая значительные их колебания во время динамического обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждому действию агента добавляется дополнительная награда, причем она обратно пропорциональна количеству раз, которые агент бывал в этом состоянии(если точнее, то количеству раз, которые агент бывал в состояниях, имеющих ту же бинарную кодировку автоэнкодера, что и текущее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "6e5eabfa443d45858c218cf7a688a2e5": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
